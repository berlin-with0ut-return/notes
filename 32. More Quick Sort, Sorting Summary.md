# 32. More Quick Sort, Sorting Summary
## Overview
### Hoare Partitioning
- fast inplace partitioning
- use pair of pointers: L and R
	- L stops at >= items, right stops at <= items
	- swap items when both stop
	- after swapping, continue moving until they cross
	- every left of left pointer <= pivot, everything right of right pivot >= pivot
	- swap pivot to appropriate location (P <--> right)
- much faster than mergesort
### Selection
- kth largest item in array
- can do with sorting in nlogn time
- PICK: linear, but still slow
### QuickSelect
- partitioning to solve selection in linear time
- partition array and quickselect on corresponding side
- runtimes: Θ(N) best/avg, Θ(N^2^) worst
### Stability
- stable = order of equal items preserved
### Optimizing Sorts
- switch to insertion for N < 15
- exploit existing order (adaptive)
	- ex: Timsort ~ Θ(N) on almost-sorted arrays
- switch to N log N sort (esp for quicksort) if recursion depth too deep
### Shuffling
- to shuffle, assign random floating point to every object, then sort
# Exercises
## C Level
### Problem 1
Problem 3  [from Hug's Fall 2014 midterm](http://datastructur.es/sp16/materials/exam/CS61B_Fall2014_MT2.pdf).
[solution](https://inst.eecs.berkeley.edu/~cs61b/fa14/test-solutions/test2-soln.pdf)
### Problem 2
Why does Java’s built-in  `Array.sort`  method use Quicksort for  `int`,  `long`,  `char`, or other primitive arrays, but Mergesort for all Object arrays?

**Ans**: Quicksort is faster than mergesort but unstable, which is fine for primitives since equal primitives cannot differ in any aspect. However, when sorting objects, equal objects may not be the same object, and stability is desireable. Hence we use mergesort, a stable algorithm.
## B Level
### Problem 1
Hug's  [Fall 2013 midterm, problem 7](http://www.cs.princeton.edu/courses/archive/fall13/cos226/exams/mid-f13.pdf), particularly part b.
[solution](https://www.cs.princeton.edu/courses/archive/fall13/cos226/exams/mid-f13-sol.pdf)
### Problem 2
Hug's [Fall 2014 midterm, problem 6](http://sp16.datastructur.es/materials/exam/CS61B_Fall2014_MT2.pdf).
[solution](https://inst.eecs.berkeley.edu/~cs61b/fa14/test-solutions/test2-soln.pdf)
## A Level
### Problem 1
Hug's [Spring 2013 midterm, problem 7](http://www.cs.princeton.edu/courses/archive/spr13/cos226/exams/mid-s13.pdf).
[solution](https://www.cs.princeton.edu/courses/archive/spr13/cos226/exams/mid-s13-sol.pdf)
### Problem 2
Given that Quick sort runs fastest if we can always somehow pick the median item as the pivot, why don’t we use Quick select to find the median to optimize our pivot selection (as opposed to using the leftmost item).

**Ans**: This actually too slow. Even though the asymptotic runtime is the same, the constant factor of quicksort increases causing a noticeable increase in the average case runtime of quicksort if we incoroporate quickselect. Selecting a random pivot is much faster. 
### Problem 3
We can make Mergesort adaptive by providing an optimization for the case where the left subarray is all smaller than the right subarray. Describe how you’d implement this optimization, and give the runtime of a merge operation for this special case.

**Ans**: You know that the left and right subarrays should be sorted by the time you get to the merging stage (recursive leap of faith). So you can check if everything in the left subarray is less than everything in the right subarray by comparing `left[left.length - 1] < right[0]`. In this case, you can simply concatenate the two arrays together and you're done sorting--no need for more recursive `mergesort` calls. 
A single merge operation still takes Θ(N) time, but now your best case mergesort is Θ(N) for a sorted array.
